{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf6dc35",
   "metadata": {},
   "source": [
    "# Avec spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "409acd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/17 08:07:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"ALSMatrixFactorisation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457024a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/17 12:23:42 WARN Instrumentation: [0d459fca] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weights:\n",
      "show_cnt: -5.4461591470311636e-08\n",
      "play_cnt: 3.653622809495512e-08\n",
      "like_user_num: -9.252525067534613e-08\n",
      "share_cnt: -2.2890577091292535e-06\n",
      "comment_cnt: -1.097557712348296e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|   popularity_score|\n",
      "+-------+-------------------+\n",
      "|  count|              10728|\n",
      "|   mean| 1.2037158926302574|\n",
      "| stddev|0.17547826921399848|\n",
      "|    min| -2.678406587847766|\n",
      "|    max|  1.276259216263778|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Load the data\n",
    "item_daily_features = spark.read.csv(\"data/item_daily_features.csv\", header=True, inferSchema=True)\n",
    "big_matrix = spark.read.csv(\"data/big_matrix.csv\", header=True, inferSchema=True)\n",
    "small_matrix = spark.read.csv(\"data/small_matrix.csv\", header=True, inferSchema=True)\n",
    "\n",
    "pop = item_daily_features.groupBy(\"video_id\").agg(\n",
    "    F.sum(\"show_cnt\").alias(\"show_cnt\"),\n",
    "    F.sum(\"play_cnt\").alias(\"play_cnt\"),\n",
    "    F.sum(\"like_user_num\").alias(\"like_user_num\"),\n",
    "    F.sum(\"share_cnt\").alias(\"share_cnt\"),\n",
    "    F.sum(\"comment_cnt\").alias(\"comment_cnt\")\n",
    ")\n",
    "\n",
    "# Compute the average watch_ratio for each video\n",
    "video_watch_ratio = big_matrix.groupBy(\"video_id\").agg(F.avg(\"watch_ratio\").alias(\"watch_ratio\"))\n",
    "\n",
    "pop = pop.join(video_watch_ratio, on=\"video_id\", how=\"right\")\n",
    "pop = pop.fillna(0)\n",
    "\n",
    "feature_cols = [\"show_cnt\", \"play_cnt\", \"like_user_num\", \"share_cnt\", \"comment_cnt\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "pop = assembler.transform(pop)\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"watch_ratio\")\n",
    "lr_model = lr.fit(pop)\n",
    "coeffs = lr_model.coefficients\n",
    "print(\"Learned weights:\")\n",
    "for feature, coeff in zip(feature_cols, coeffs):\n",
    "    print(f\"{feature}: {coeff}\")\n",
    "\n",
    "pop = lr_model.transform(pop).withColumnRenamed(\"prediction\", \"popularity_score\")\n",
    "pop_df = pop.select(\"video_id\", \"popularity_score\")\n",
    "pop_df.describe(\"popularity_score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75684c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------+--------------+--------------------+--------+----------------+------------------+\n",
      "|user_id|video_id|play_duration|video_duration|                time|    date|       timestamp|       watch_ratio|\n",
      "+-------+--------+-------------+--------------+--------------------+--------+----------------+------------------+\n",
      "|      0|    3649|        13838|         10867|2020-07-05 00:08:...|20200705|1.593878903438E9|1.2733965215790926|\n",
      "|      0|    9598|        13665|         10984|2020-07-05 00:13:...|20200705|1.593879221297E9|1.2440823015294975|\n",
      "|      0|    5262|          851|          7908|2020-07-05 00:16:...|20200705|1.593879366687E9|0.1076125442589782|\n",
      "|      0|    1963|          862|          9590|2020-07-05 00:20:...|20200705|1.593879626792E9|0.0898852971845672|\n",
      "|      0|    8234|          858|         11000|2020-07-05 00:43:...|20200705|1.593880985128E9|             0.078|\n",
      "+-------+--------+-------------+--------------+--------------------+--------+----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "big_matrix.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0545d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min watch ratio: 0.0\n",
      "Max watch ratio: 573.4571428571429\n",
      "Mean watch ratio: 0.944505920574192\n",
      "Std watch ratio: 1.6746010308958716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "min_watch_ratio = big_matrix.agg({\"watch_ratio\": \"min\"}).collect()[0][0]\n",
    "max_watch_ratio = big_matrix.agg({\"watch_ratio\": \"max\"}).collect()[0][0]\n",
    "mean_watch_ratio = big_matrix.agg({\"watch_ratio\": \"avg\"}).collect()[0][0]\n",
    "std_watch_ratio = big_matrix.agg({\"watch_ratio\": \"stddev\"}).collect()[0][0]\n",
    "\n",
    "print(f\"Min watch ratio: {min_watch_ratio}\")\n",
    "print(f\"Max watch ratio: {max_watch_ratio}\")\n",
    "print(f\"Mean watch ratio: {mean_watch_ratio}\")\n",
    "print(f\"Std watch ratio: {std_watch_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c16cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import log1p\n",
    "\n",
    "interactions = big_matrix.select(\n",
    "    col(\"user_id\").cast(\"int\"),\n",
    "    col(\"video_id\").cast(\"int\"),\n",
    "    col(\"watch_ratio\").cast(\"double\")\n",
    ").na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77040e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "user_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"userIndex\").fit(interactions)\n",
    "interactions = user_indexer.transform(interactions)\n",
    "item_indexer = StringIndexer(inputCol=\"video_id\", outputCol=\"videoIndex\").fit(interactions)\n",
    "interactions = item_indexer.transform(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ca4594",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = interactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4dc28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb lignes original: 12530806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb lignes filtré: 9101961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------------------+---------+----------+\n",
      "|video_id|user_id|       watch_ratio|userIndex|videoIndex|\n",
      "+--------+-------+------------------+---------+----------+\n",
      "|    9900|    148|1.1622798529127154|   1218.0|    1121.0|\n",
      "|     471|    148|1.0172131147540984|   1218.0|     886.0|\n",
      "|    6357|    148|0.1304439390110353|   1218.0|     656.0|\n",
      "|    7880|    148|0.1615575396825397|   1218.0|     469.0|\n",
      "|    7880|    148|0.1615575396825397|   1218.0|     469.0|\n",
      "+--------+-------+------------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning and filtering\n",
    "from pyspark.sql.functions import col, count\n",
    "from pyspark.sql.window import Window\n",
    "user_inter_counts = (\n",
    "    train.groupBy(\"user_id\")\n",
    "    .agg(count(\"video_id\").alias(\"user_interactions\"))\n",
    ")\n",
    "video_inter_counts = (\n",
    "    train.groupBy(\"video_id\")\n",
    "    .agg(count(\"user_id\").alias(\"video_interactions\"))\n",
    ")\n",
    "quantile_75 = (\n",
    "    train.approxQuantile(\"watch_ratio\", [0.75], 0.01)[0]\n",
    ")\n",
    "\n",
    "filtered_train = (\n",
    "    train\n",
    "    .join(\n",
    "        user_inter_counts.filter(col(\"user_interactions\") >= 248),\n",
    "        on=\"user_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .join(\n",
    "        video_inter_counts.filter(col(\"video_interactions\") >= 1),\n",
    "        on=\"video_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .filter(\n",
    "        (col(\"watch_ratio\") > 0) & \n",
    "        (col(\"watch_ratio\") <= quantile_75)\n",
    "    )\n",
    "    .drop(\"user_interactions\", \"video_interactions\")\n",
    ")\n",
    "\n",
    "print(f\"Nb lignes original: {train.count()}\")\n",
    "print(f\"Nb lignes filtré: {filtered_train.count()}\")\n",
    "filtered_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8092038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn(\"watch_ratio\", log1p(col(\"watch_ratio\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3adb08b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test = small_matrix.select(\n",
    "    col(\"user_id\").cast(\"int\"),\n",
    "    col(\"video_id\").cast(\"int\"),\n",
    "    col(\"watch_ratio\").cast(\"double\")\n",
    ").na.fill(0) \n",
    "test = test.withColumn(\"watch_ratio\", log1p(col(\"watch_ratio\")))\n",
    "user_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"userIndex\").fit(test)\n",
    "test = user_indexer.transform(test)\n",
    "item_indexer = StringIndexer(inputCol=\"video_id\", outputCol=\"videoIndex\").fit(test)\n",
    "test = item_indexer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb443424",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.join(pop_df, on=\"video_id\", how=\"left\")\n",
    "test = test.join(pop_df, on=\"video_id\", how=\"left\")\n",
    "train = train.fillna({\"popularity_score\": 0})\n",
    "test = test.fillna({\"popularity_score\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb308ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/17 08:08:06 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/05/17 08:08:06 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "25/05/17 08:08:06 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/05/17 08:08:07 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "als_log = ALS(\n",
    "    maxIter=20,\n",
    "    regParam=0.01,\n",
    "    rank=10,\n",
    "    userCol=\"userIndex\",\n",
    "    itemCol=\"videoIndex\",\n",
    "    ratingCol=\"watch_ratio\",\n",
    "    implicitPrefs=False,\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "model_log = als_log.fit(filtered_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e16140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_log.transform(test)\n",
    "predictions = predictions.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5571e2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE@10: 0.8481\n",
      "RMSE@10: 0.9492\n",
      "NDCG@10: 0.8603\n",
      "Novelty@10: 1.9056\n",
      "Average Popularity@10: 1.0628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import expm1\n",
    "from pyspark.sql.window import Window\n",
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def evaluate_model_with_continuous_precision_spark(model, df_test, df_train, user_feat, k=10):\n",
    "    item_popularity_df = df_train.groupBy(\"video_id\").count().withColumnRenamed(\"count\", \"popularity\")\n",
    "    total_items = df_train.select(\"video_id\").distinct().count()\n",
    "\n",
    "    df_test = df_test.join(user_feat, on=\"user_id\", how=\"left\")\n",
    "\n",
    "    pred_df = model.transform(test)\n",
    "    pred_df = predictions.na.drop()\n",
    "    pred_df = predictions.withColumn(\"prediction\", expm1(col(\"prediction\")))\n",
    "\n",
    "    # Ranking\n",
    "    windowSpec = Window.partitionBy(\"user_id\").orderBy(F.desc(\"prediction\"))\n",
    "    pred_df = pred_df.withColumn(\"rank\", F.row_number().over(windowSpec))\n",
    "    pred_top_k = pred_df.filter(F.col(\"rank\") <= k)\n",
    "\n",
    "    # MAE & RMSE\n",
    "    pred_top_k = pred_top_k.withColumn(\"abs_error\", F.abs(F.col(\"watch_ratio\") - F.col(\"prediction\")))\n",
    "    pred_top_k = pred_top_k.withColumn(\"sq_error\", (F.col(\"watch_ratio\") - F.col(\"prediction\")) ** 2)\n",
    "\n",
    "    mae = pred_top_k.groupBy().agg(F.avg(\"abs_error\")).collect()[0][0]\n",
    "    rmse = math.sqrt(pred_top_k.groupBy().agg(F.avg(\"sq_error\")).collect()[0][0])\n",
    "\n",
    "    # NDCG\n",
    "    top_k_collected = pred_top_k.select(\"user_id\", \"video_id\", \"prediction\", \"watch_ratio\").groupBy(\"user_id\").agg(\n",
    "        F.collect_list(\"video_id\").alias(\"videos\"),\n",
    "        F.collect_list(\"prediction\").alias(\"preds\"),\n",
    "        F.collect_list(\"watch_ratio\").alias(\"truths\")\n",
    "    ).collect()\n",
    "\n",
    "    ndcg_list = []\n",
    "    for row in top_k_collected:\n",
    "        ndcg_list.append(ndcg_score([row[\"truths\"]], [row[\"preds\"]]))\n",
    "\n",
    "    avg_ndcg = np.mean(ndcg_list)\n",
    "\n",
    "    # Novelty\n",
    "    pred_top_k = pred_top_k.join(item_popularity_df, on=\"video_id\", how=\"left\")\n",
    "    pred_top_k = pred_top_k.withColumn(\"novelty\", -F.log2(F.col(\"popularity\") / total_items + 1e-10))\n",
    "    novelty = pred_top_k.groupBy().agg(F.avg(\"novelty\")).collect()[0][0]\n",
    "\n",
    "    # Popularity@k\n",
    "    pred_top_k = pred_top_k.join(pop_df, on=\"video_id\", how=\"left\")\n",
    "    avg_popularity = pred_top_k.groupBy().agg(F.avg(\"popularity_score\")).collect()[0][0]\n",
    "    \n",
    "    # Résultats\n",
    "    print(f'MAE@{k}: {mae:.4f}')\n",
    "    print(f'RMSE@{k}: {rmse:.4f}')\n",
    "    print(f'NDCG@{k}: {avg_ndcg:.4f}')\n",
    "    print(f'Novelty@{k}: {novelty:.4f}')\n",
    "    print(f'Average Popularity@{k}: {avg_popularity:.4f}')\n",
    "\n",
    "user_feat = spark.read.csv(\"data/user_features.csv\", header=True, inferSchema=True)\n",
    "evaluate_model_with_continuous_precision_spark(\n",
    "    model_log, test, train, user_feat, k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f5f5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+----------------+----------------+\n",
      "|videoIndex|userIndex|watch_ratio|original_user_id|original_item_id|\n",
      "+----------+---------+-----------+----------------+----------------+\n",
      "|      9719|      496|  1.3478788|            3389|            6366|\n",
      "|     10328|      496|   1.284718|            3389|            4003|\n",
      "|      9985|      496|   1.244511|            3389|            1754|\n",
      "|      9081|      496|  1.2115295|            3389|           10355|\n",
      "|      9738|      496|  1.1401483|            3389|            9081|\n",
      "|      9341|      496|  1.1370603|            3389|            1451|\n",
      "|      8007|      496|  1.1298721|            3389|            6097|\n",
      "|      8371|      496|  1.1215873|            3389|            9948|\n",
      "|      9371|      496|  1.1193432|            3389|            6200|\n",
      "|      7998|      496|  1.1104711|            3389|            3480|\n",
      "+----------+---------+-----------+----------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get top 10 recommendations for each user\n",
    "user_recommendations = model_log.recommendForAllUsers(10)\n",
    "user_recommendations = user_recommendations.withColumn(\n",
    "    \"recommendations\",\n",
    "    F.explode(\"recommendations\")\n",
    ").select(\n",
    "    col(\"userIndex\"),\n",
    "    col(\"recommendations.videoIndex\").alias(\"videoIndex\"),\n",
    "    col(\"recommendations.rating\").alias(\"watch_ratio\")\n",
    ")\n",
    "user_converter = interactions.select(\"userIndex\", \"user_id\").distinct().withColumnRenamed(\"user_id\", \"original_user_id\")\n",
    "item_converter = interactions.select(\"videoIndex\", \"video_id\").distinct().withColumnRenamed(\"video_id\", \"original_item_id\")\n",
    "user_recommendations = user_recommendations.join(\n",
    "    user_converter.select(col(\"userIndex\"), col(\"original_user_id\")),\n",
    "    on=\"userIndex\",\n",
    "    how=\"inner\"\n",
    ").join(\n",
    "    item_converter.select(col(\"videoIndex\"), col(\"original_item_id\")),\n",
    "    on=\"videoIndex\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "user_recommendations.show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
